{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5e6536",
   "metadata": {},
   "source": [
    "### Mendefinisikan class dan fungsi Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.stemmer = StemmerFactory().create_stemmer()\n",
    "        stopword_factory = StopWordRemoverFactory()\n",
    "        self.combined_stopwords = set(stopword_factory.get_stop_words()).union(set(stopwords.words('english')))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'[\\\"“”]', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text) -> str:\n",
    "        text = self.clean_text(text).lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.combined_stopwords]\n",
    "        stemmed = [self.stemmer.stem(word) for word in tokens]\n",
    "        processed_text = ' '.join(stemmed)\n",
    "        return processed_text\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, dataframe: pd.DataFrame, columns: list):\n",
    "        self.dataframe = dataframe\n",
    "        self.columns = columns\n",
    "        self.cleaner = TextCleaner()\n",
    "\n",
    "    def exec(self) -> pd.DataFrame:\n",
    "        df_copy = self.dataframe.copy()\n",
    "        for col in self.columns:\n",
    "            if col in df_copy.columns:\n",
    "                df_copy[col] = df_copy[col].astype(str).apply(self.cleaner.preprocess_text)\n",
    "        logging.info('> Preprocessing completed.')\n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f9238",
   "metadata": {},
   "source": [
    "### Mendefinisikan class dan fungsi TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95707d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import math\n",
    "import logging\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class TFIDFVectorizerExporter:\n",
    "    def __init__(self, input_df: pd.DataFrame, output_file: str, text_column: str, model_output:str):\n",
    "        if not isinstance(input_df, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a DataFrame.\")\n",
    "\n",
    "        if text_column not in input_df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "        self.input_df = input_df\n",
    "        self.output_file = output_file\n",
    "        self.text_column = text_column\n",
    "        self.model_output = model_output\n",
    "\n",
    "    def export_tfidf(self):\n",
    "        try:\n",
    "            self.input_df[self.text_column] = self.input_df[self.text_column].fillna(\"\").astype(str)\n",
    "\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = vectorizer.fit_transform(self.input_df[self.text_column])\n",
    "\n",
    "            joblib.dump(vectorizer, self.model_output)\n",
    "\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "            tfidf_df.to_csv(self.output_file, index=False)\n",
    "\n",
    "            logging.info(f\"TF-IDF Vectorizer successfully exported to {self.output_file}\")\n",
    "\n",
    "            return tfidf_matrix, tfidf_df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"Error processing data: {ve}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "class TFIDFCalculator:\n",
    "    def __init__(self, preprocessed_df: pd.DataFrame, output_file: str, text_column: str):\n",
    "        if not isinstance(preprocessed_df, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a DataFrame.\")\n",
    "\n",
    "        if text_column not in preprocessed_df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "        self.preprocessed_df = preprocessed_df\n",
    "        self.output_file = output_file\n",
    "        self.text_column = text_column\n",
    "        self.documents = []\n",
    "        self.terms = set()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.documents = self.preprocessed_df[self.text_column].tolist()\n",
    "        self.terms = set(\" \".join(self.documents).split())\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "\n",
    "    def calculate_tf(self):\n",
    "        term_frequencies = []\n",
    "        for doc in self.documents:\n",
    "            counter = Counter(doc.split())\n",
    "            total_terms = sum(counter.values())\n",
    "            tf_raw = {term: counter.get(term, 0) for term in self.terms}\n",
    "            if total_terms > 0:\n",
    "                tf_norm = {\n",
    "                    term: round(count / total_terms, 4) if total_terms != 0 else 0\n",
    "                    for term, count in tf_raw.items()\n",
    "                }\n",
    "            else:\n",
    "                tf_norm = {term: 0 for term in self.terms}\n",
    "            term_frequencies.append((tf_raw, tf_norm))\n",
    "        logging.info(\"Term frequencies calculated successfully.\")\n",
    "        return term_frequencies\n",
    "\n",
    "    def calculate_df(self):\n",
    "        df = {term: 0 for term in self.terms}\n",
    "        for doc in self.documents:\n",
    "            for term in set(doc.split()):\n",
    "                df[term] += 1\n",
    "        logging.info(\"Document frequencies calculated successfully.\")\n",
    "        return df\n",
    "\n",
    "    def calculate_idf(self, df: dict) -> dict:\n",
    "        num_docs = len(self.documents)\n",
    "        idf = {\n",
    "            term: round(math.log((1 + num_docs) / (1 + df[term])) + 1, 4)\n",
    "            for term in self.terms\n",
    "        }\n",
    "        logging.info(\"Inverse document frequencies calculated successfully.\")\n",
    "        return idf\n",
    "\n",
    "    def calculate_tfidf(self, term_frequencies, idf):\n",
    "        tfidf = []\n",
    "        tfidf_norm = []\n",
    "        for tf_raw, tf_norm in term_frequencies:\n",
    "            tfidf_doc = {\n",
    "                term: round(tf_norm[term] * idf[term], 4) for term in self.terms\n",
    "            }\n",
    "            norm = math.sqrt(sum(value**2 for value in tfidf_doc.values()))\n",
    "            tfidf_norm_doc = (\n",
    "                {term: round(value / norm, 4) for term, value in tfidf_doc.items()}\n",
    "                if norm != 0\n",
    "                else {term: 0 for term in self.terms}\n",
    "            )\n",
    "            tfidf.append(tfidf_doc)\n",
    "            tfidf_norm.append(tfidf_norm_doc)\n",
    "        logging.info(\"TF-IDF values calculated successfully.\")\n",
    "        return tfidf, tfidf_norm\n",
    "\n",
    "    def export_results(self, term_frequencies, df, idf, tfidf, tfidf_norm):\n",
    "        with open(self.output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            header = (\n",
    "                [\"Term\"]\n",
    "                + [f\"TF{i + 1}\" for i in range(len(self.documents))]\n",
    "                + [f\"TFN{i + 1}\" for i in range(len(self.documents))]\n",
    "                + [\"DF\", \"IDF\"]\n",
    "                + [f\"TFIDF{i + 1}\" for i in range(len(self.documents))]\n",
    "                + [f\"TFIDFN{i + 1}\" for i in range(len(self.documents))]\n",
    "            )\n",
    "            writer.writerow(header)\n",
    "\n",
    "            for term in sorted(self.terms):\n",
    "                row = [term]\n",
    "                row += [doc_tf[0][term] for doc_tf in term_frequencies]\n",
    "                row += [doc_tf[1][term] for doc_tf in term_frequencies]\n",
    "                row.append(df[term])\n",
    "                row.append(idf[term])\n",
    "                row += [doc_tfidf[term] for doc_tfidf in tfidf]\n",
    "                row += [doc_tfidf_norm[term] for doc_tfidf_norm in tfidf_norm]\n",
    "                writer.writerow(row)\n",
    "        logging.info(f\"Results successfully exported to {self.output_file}\")\n",
    "\n",
    "    def process(self):\n",
    "        self.load_data()\n",
    "        term_frequencies = self.calculate_tf()\n",
    "        df = self.calculate_df()\n",
    "        idf = self.calculate_idf(df)\n",
    "        tfidf, tfidf_norm = self.calculate_tfidf(term_frequencies, idf)\n",
    "        self.export_results(term_frequencies, df, idf, tfidf, tfidf_norm)\n",
    "        logging.info(\"TF-IDF calculation and export process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8d8dc",
   "metadata": {},
   "source": [
    "### Mendefinisikan class dan fungsi SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def prepare_train_model(\n",
    "    y_train_output,\n",
    "    dataset: pd.DataFrame,\n",
    "    dataset_colname: str = 'label'):\n",
    "    logging.info(\"===== Starting Model Preparation =====\")\n",
    "\n",
    "    # Load dataset\n",
    "    documents_sentiment = list(dataset[dataset_colname])\n",
    "    logging.info(\"Loaded dataset with %d documents\", len(documents_sentiment))\n",
    "\n",
    "    # Label encoding (convert labels to numbers)\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(documents_sentiment)\n",
    "\n",
    "    # Save encoder\n",
    "    encoder_path = y_train_output\n",
    "    joblib.dump(encoder, f\"{encoder_path}/encoder.pkl\")\n",
    "    logging.info(\"Label encoder saved to %s/encoder.pkl\", encoder_path)\n",
    "\n",
    "    logging.info(\"Pre-train dataset successfully saved\")\n",
    "\n",
    "    return y_train\n",
    "\n",
    "\n",
    "def train_model(kernel: str, X_train: pd.DataFrame, y_train: pd.Series, model_output_pathandname: str):\n",
    "    if kernel not in ['sigmoid', 'linear', 'rbf', 'poly']:\n",
    "        logging.error(\"Invalid kernel: %s\", kernel)\n",
    "        raise ValueError(\"Kernel must be one of: 'sigmoid', 'linear', 'rbf', 'poly'\")\n",
    "\n",
    "    model = SVC(kernel=kernel)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model, model_output_pathandname)\n",
    "    logging.info(\"Model trained and saved to %s\", model_output_pathandname)\n",
    "\n",
    "def predict(test_text: str, X_trained_pkl, svm_model_pkl):\n",
    "    logging.info(\"📌 Memulai proses prediksi...\")\n",
    "\n",
    "    # Pastikan file model tersedia\n",
    "    if not X_trained_pkl.exists():\n",
    "        logging.error(f\"🚨 Model TF-IDF {X_trained_pkl} tidak ditemukan.\")\n",
    "        return None\n",
    "\n",
    "    if not svm_model_pkl.exists():\n",
    "        logging.error(f\"🚨 Model SVM {svm_model_pkl} tidak ditemukan.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Preprocessing teks input\n",
    "        preprocessor = TextCleaner()\n",
    "        preprocessed_text = preprocessor.preprocess_text(test_text)\n",
    "\n",
    "        # Load TF-IDF model\n",
    "        logging.info(\"🔄 Memuat model TF-IDF...\")\n",
    "        tfidf_model = joblib.load(X_trained_pkl)\n",
    "        X_test = tfidf_model.transform([preprocessed_text])\n",
    "\n",
    "        # Konversi ke dense array jika masih berbentuk sparse matrix\n",
    "        if hasattr(X_test, \"toarray\"):\n",
    "            X_test = X_test.toarray()\n",
    "\n",
    "        # Load model SVM\n",
    "        logging.info(f\"🔄 Memuat model SVM dari {svm_model_pkl.stem}...\")\n",
    "        svm_model = joblib.load(svm_model_pkl)\n",
    "\n",
    "        # Pastikan model memiliki metode predict\n",
    "        if not hasattr(svm_model, \"predict\"):\n",
    "            logging.error(\"🚨 Model yang dimuat bukan model SVM yang valid.\")\n",
    "            return None\n",
    "\n",
    "        # Lakukan prediksi\n",
    "        prediction = svm_model.predict(X_test)\n",
    "        logging.info(f\"✅ Hasil prediksi: {prediction[0]}\")\n",
    "\n",
    "        return prediction[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"🚨 Terjadi kesalahan saat melakukan prediksi: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_and_export_results(model, X_test, y_test, kernel: str, output_csv_path: str):\n",
    "    logging.info(\"📊 Mengevaluasi performa model SVM...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    result = {\n",
    "        \"Kernel\": kernel,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1_Score\": f1,\n",
    "        \"Inference_Time_Seconds\": inference_time\n",
    "    }\n",
    "\n",
    "    file_exists = Path(output_csv_path).exists()\n",
    "\n",
    "    # Tulis ke CSV (append jika file sudah ada)\n",
    "    with open(output_csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=result.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(result)\n",
    "\n",
    "    logging.info(\"📁 Hasil evaluasi diekspor ke %s\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad27ffd",
   "metadata": {},
   "source": [
    "## Latih model SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718ac29",
   "metadata": {},
   "source": [
    "#### Persiapan data, folder, dan fungsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Konfigurasi Path**\n",
    "# ----------------------------------------------------------------\n",
    "BASE_DIR = Path().resolve()\n",
    "\n",
    "# Direktori penyimpanan hasil\n",
    "result_path = BASE_DIR / \"result\"\n",
    "preprocessed_path = result_path / \"preprocessed\"\n",
    "tfidf_path = result_path / \"tfidf\"\n",
    "model_path = result_path / \"model\"\n",
    "predict_path = result_path / \"predict\"\n",
    "\n",
    "# Buat folder jika belum ada\n",
    "for path in [result_path, preprocessed_path, tfidf_path, model_path, predict_path]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi Preprocessing**\n",
    "# ----------------------------------------------------------------\n",
    "def preprocessing():\n",
    "    print('Preprocessing.......')\n",
    "    df = pd.read_csv(BASE_DIR / \"dataset.csv\")\n",
    "    preprocessor = DataPreprocessing(df, ['comment'])\n",
    "    preprocessed_data = preprocessor.exec()\n",
    "\n",
    "    # Simpan hasil ke CSV\n",
    "    preprocessed_file = preprocessed_path / \"preprocessed.csv\"\n",
    "    preprocessed_data.to_csv(preprocessed_file, index=False)\n",
    "    logging.info(f\"✅ Preprocessed data disimpan di {preprocessed_file}\")\n",
    "    print('preprocessing done')\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi Training TF-IDF**\n",
    "# ----------------------------------------------------------------\n",
    "def training_tfidf(train_size=0.8):\n",
    "    print(\"📌 Memulai training TF-IDF...\")\n",
    "\n",
    "    # Membaca data yang telah dipreproses\n",
    "    data_preprocessed = pd.read_csv(preprocessed_path / \"preprocessed.csv\")\n",
    "\n",
    "    # Split data menjadi training dan testing\n",
    "    df_train, df_test = train_test_split(\n",
    "        data_preprocessed, train_size=train_size, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Simpan hasil split ke dalam file CSV\n",
    "    training_file = preprocessed_path / \"training.csv\"\n",
    "    testing_file = preprocessed_path / \"testing.csv\"\n",
    "    df_train.to_csv(training_file, index=False)\n",
    "    df_test.to_csv(testing_file, index=False)\n",
    "\n",
    "    logging.info(f\"✅ Data training disimpan di {training_file}\")\n",
    "    logging.info(f\"✅ Data testing disimpan di {testing_file}\")\n",
    "\n",
    "    # TF-IDF Vectorizer menggunakan data training\n",
    "    tfidf_vec_file = tfidf_path / \"tfidf_vec.csv\"\n",
    "    X_train_pkl = model_path / \"X_trained.pkl\"\n",
    "\n",
    "    tfidf_vec = TFIDFVectorizerExporter(df_train, tfidf_vec_file, \"comment\", X_train_pkl)\n",
    "    _, tfidf_df = tfidf_vec.export_tfidf()\n",
    "\n",
    "    logging.info(f\"✅ TF-IDF vectorized data disimpan di {tfidf_vec_file}\")\n",
    "\n",
    "    # TF-IDF Manual Calculation\n",
    "    tfidf_manual_file = tfidf_path / \"tfidf_manual.csv\"\n",
    "    tfidf_manual = TFIDFCalculator(df_train, tfidf_manual_file, \"comment\")\n",
    "    tfidf_manual.process()\n",
    "\n",
    "    logging.info(f\"TF-IDF manual calculation disimpan di {tfidf_manual_file}\")\n",
    "    print(\"✅ Training TF-IDF selesai.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi Training SVM (Dinamically Train All Models)**\n",
    "# ----------------------------------------------------------------\n",
    "def training_svm(kernel: str):\n",
    "    print(\"📌 Memulai training SVM...\")\n",
    "\n",
    "    # Membaca data training hasil split\n",
    "    data_train = pd.read_csv(preprocessed_path / \"training.csv\")\n",
    "\n",
    "    # Persiapan label (y_train)\n",
    "    y_train = prepare_train_model(model_path, data_train)\n",
    "\n",
    "    # Membaca TF-IDF matrix hasil training\n",
    "    tfidf_matrix = pd.read_csv(tfidf_path / \"tfidf_vec.csv\")\n",
    "\n",
    "    # Training dan simpan model\n",
    "    model_file = model_path / f\"{kernel}.pkl\"\n",
    "    train_model(kernel, tfidf_matrix, y_train, model_file)\n",
    "\n",
    "    logging.info(f\"✅ Model {kernel} disimpan di {model_file}\")\n",
    "    print(\"✅ Training SVM selesai.\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi Prediksi (Dinamis)**\n",
    "# ----------------------------------------------------------------\n",
    "def run_prediction(test_text, model_name=None):\n",
    "    \"\"\"Menjalankan prediksi teks dengan model SVM\"\"\"\n",
    "    logging.info(\"📌 Memulai prediksi...\")\n",
    "\n",
    "    # Load Model\n",
    "    svm_model, model_filename = load_model(model_name)\n",
    "\n",
    "    # Pastikan TF-IDF sudah tersedia\n",
    "    X_trained_pkl = model_path / \"X_trained.pkl\"\n",
    "    if not X_trained_pkl.exists():\n",
    "        logging.error(\"🚨 File TF-IDF (X_trained.pkl) tidak ditemukan. Silakan lakukan training terlebih dahulu.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # File output hasil prediksi\n",
    "    prediction_output = predict_path / f\"predict_{model_filename}.csv\"\n",
    "\n",
    "    # Jalankan prediksi\n",
    "    result = predict(test_text, X_trained_pkl, svm_model, prediction_output)\n",
    "\n",
    "    logging.info(f\"✅ Hasil prediksi ({model_filename}): {result}\")\n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi Evaluasi Akurasi Model**\n",
    "# ----------------------------------------------------------------\n",
    "def evaluate_model():\n",
    "    logging.info(\"📌 Memulai evaluasi model...\")\n",
    "\n",
    "    # Load data testing dari file CSV, bukan dari database\n",
    "    testing_file = preprocessed_path / \"testing.csv\"\n",
    "    if not testing_file.exists():\n",
    "        logging.error(\"🚨 File testing.csv tidak ditemukan. Jalankan training_tfidf() terlebih dahulu.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    data_testing = pd.read_csv(testing_file)\n",
    "\n",
    "    if data_testing.empty:\n",
    "        logging.error(\"🚨 Data testing.csv kosong. Silakan pastikan data tersedia.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    X_test = data_testing[\"comment\"]\n",
    "    y_test = data_testing[\"label\"]\n",
    "\n",
    "    # Load LabelEncoder\n",
    "    encoder_pkl = model_path / \"encoder.pkl\"\n",
    "    if not encoder_pkl.exists():\n",
    "        logging.error(\"🚨 File encoder.pkl tidak ditemukan. Pastikan telah melakukan training.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    label_encoder = joblib.load(encoder_pkl)\n",
    "\n",
    "    # Encode label testing sesuai dengan training\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    # Generate wordcloud dari data uji\n",
    "    generate_wordcloud(X_test, y_test_encoded, label_encoder)\n",
    "\n",
    "    # Load TF-IDF vectorizer hasil training\n",
    "    X_train_pkl = model_path / \"X_trained.pkl\"\n",
    "    if not X_train_pkl.exists():\n",
    "        logging.error(\"🚨 File TF-IDF vectorizer tidak ditemukan. Pastikan proses training telah dilakukan.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    tfidf_vectorizer = joblib.load(X_train_pkl)\n",
    "\n",
    "    # Transformasi TF-IDF untuk data uji\n",
    "    X_test_tfidf_array = tfidf_vectorizer.transform(X_test)\n",
    "    X_test_tfidf = pd.DataFrame(\n",
    "        X_test_tfidf_array.toarray(),\n",
    "        columns=tfidf_vectorizer.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "\n",
    "    # Cek model-model yang tersedia\n",
    "    available_models = [\n",
    "        m for m in model_path.glob(\"*.pkl\")\n",
    "        if \"encoder\" not in m.stem and \"X_trained\" not in m.stem\n",
    "    ]\n",
    "\n",
    "    if not available_models:\n",
    "        logging.error(\"🚨 Tidak ada model SVM yang tersedia. Pastikan telah melakukan training.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_file in available_models:\n",
    "        model_name = model_file.stem\n",
    "        logging.info(f\"🔍 Evaluasi model: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            model = joblib.load(model_file)\n",
    "\n",
    "            if not hasattr(model, \"predict\"):\n",
    "                logging.warning(f\"⚠️ {model_name} bukan model SVM yang valid. Melewati...\")\n",
    "                continue\n",
    "\n",
    "            y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "            accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "            precision = precision_score(y_test_encoded, y_pred, average=\"weighted\")\n",
    "            recall = recall_score(y_test_encoded, y_pred, average=\"weighted\")\n",
    "            f1 = f1_score(y_test_encoded, y_pred, average=\"weighted\")\n",
    "            report = classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "            results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1-Score\": f1\n",
    "            })\n",
    "\n",
    "            eval_file = predict_path / f\"evaluation_{model_name}.txt\"\n",
    "            with open(eval_file, \"w\") as f:\n",
    "                f.write(f\"Model: {model_name}\\n\")\n",
    "                f.write(f\"Akurasi: {accuracy:.4f}\\n\")\n",
    "                f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "                f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "                f.write(f\"F1-Score: {f1:.4f}\\n\\n\")\n",
    "                f.write(\"Classification Report:\\n\")\n",
    "                f.write(report)\n",
    "\n",
    "            logging.info(f\"✅ Evaluasi selesai untuk model {model_name}\")\n",
    "            logging.info(f\"📄 Hasil disimpan di {eval_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"🚨 Error saat evaluasi model {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if results:\n",
    "        plot_evaluation_results(results)\n",
    "\n",
    "    logging.info(\"✅ Evaluasi semua model selesai.\")\n",
    "    print(\"Evaluasi selesai\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi untuk Membuat Grafik Evaluasi**\n",
    "# ----------------------------------------------------------------\n",
    "def plot_evaluation_results(results):\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Set style seaborn untuk tampilan yang lebih rapi\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Plot hasil evaluasi dalam bentuk bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df_results.set_index(\"Model\").plot(kind=\"bar\", ax=ax, colormap=\"coolwarm\")\n",
    "\n",
    "    # Tambahkan angka di atas setiap bar\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", fontsize=10, padding=3)\n",
    "\n",
    "    # Tambahkan judul dan label\n",
    "    plt.title(\"Evaluasi Model: Akurasi, Precision, Recall, F1-Score\", fontsize=14)\n",
    "    plt.xlabel(\"Model\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)  # Karena semua metrik berada dalam range 0-1\n",
    "\n",
    "    # Simpan gambar\n",
    "    eval_img_path = predict_path / \"model_evaluation.png\"\n",
    "    plt.savefig(eval_img_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"📊 Grafik evaluasi disimpan di {eval_img_path}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# **Fungsi untuk Membuat WordCloud berdasarkan Kategori Sentimen**\n",
    "# ----------------------------------------------------------------\n",
    "def generate_wordcloud(comments, labels, label_encoder):\n",
    "    logging.info(\"📌 Membuat WordCloud berdasarkan kategori sentimen...\")\n",
    "\n",
    "    # Pastikan labels dikonversi ke format string (jika belum)\n",
    "    labels = label_encoder.inverse_transform(labels)\n",
    "\n",
    "    # Kategorisasi komentar berdasarkan label\n",
    "    categories = {\n",
    "        \"Positif\": [],\n",
    "        \"Netral\": [],\n",
    "        \"Negatif\": []\n",
    "    }\n",
    "\n",
    "    for comment, label in zip(comments, labels):\n",
    "        if label.lower() == \"positif\":\n",
    "            categories[\"Positif\"].append(comment)\n",
    "        elif label.lower() == \"netral\":\n",
    "            categories[\"Netral\"].append(comment)\n",
    "        elif label.lower() == \"negatif\":\n",
    "            categories[\"Negatif\"].append(comment)\n",
    "\n",
    "    # Warna berbeda untuk tiap kategori\n",
    "    colormaps = {\n",
    "        \"Positif\": \"Greens\",\n",
    "        \"Netral\": \"Blues\",\n",
    "        \"Negatif\": \"Reds\"\n",
    "    }\n",
    "\n",
    "    # Buat dan simpan WordCloud untuk setiap kategori\n",
    "    for category, texts in categories.items():\n",
    "        if texts:  # Pastikan ada teks untuk diproses\n",
    "            text_combined = \" \".join(texts)\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap=colormaps[category]).generate(text_combined)\n",
    "\n",
    "            # Plot dan simpan gambar\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"WordCloud - {category}\", fontsize=14)\n",
    "\n",
    "            # Simpan gambar\n",
    "            wordcloud_path = predict_path / f\"wordcloud_{category.lower()}.png\"\n",
    "            plt.savefig(wordcloud_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "            logging.info(f\"📊 WordCloud {category} disimpan di {wordcloud_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1ef61",
   "metadata": {},
   "source": [
    "##### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b5ac4",
   "metadata": {},
   "source": [
    "##### Training TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tfidf(train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a666d",
   "metadata": {},
   "source": [
    "##### Training SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_svm('linear')\n",
    "training_svm('rbf')\n",
    "training_svm('poly')\n",
    "training_svm('sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb3328",
   "metadata": {},
   "source": [
    "##### Evaluasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
